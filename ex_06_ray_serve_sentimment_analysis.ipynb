{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f37ff9e-4173-41ab-8e1e-6dc84a15aefb",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "We want to do sentiment analysis by using [VaderSentiment](https://github.com/cjhutto/vaderSentiment) ML framework. The goal of sentiment analysis is to \"gauge the attitudes, sentiments, evaluations, and emotions of a speaker/writer based on the computational treatment of subjectivity in a text.\"\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "\n",
    "VADER has a lot of advantages over traditional methods of Sentiment Analysis, including:\n",
    "\n",
    " * It works exceedingly well on social media type text, yet readily generalizes to multiple domains\n",
    " * It doesn’t require any training data but is constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon\n",
    " * It is fast enough to be used online with streaming data, and\n",
    " * It does not severely suffer from a speed-performance tradeoff.\n",
    " \n",
    " <table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://github.com/dmatrix/olt-mlflow/raw/master/models/images/sentiment_analysis.jpg\"\n",
    "         alt=\"Sentiment Analysis with Vader\" height=\"400 width=\"600\">\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "[image source](https://medium.com/analytics-vidhya/sentiment-analysis-with-vader-label-the-unlabeled-data-8dd785225166)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aee613-8dae-4bd4-a5b1-03cac76a2686",
   "metadata": {},
   "source": [
    "Need to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b49d5c5-181b-4901-8841-201d21260004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (from requests->vaderSentiment) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (from requests->vaderSentiment) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/ray-core/lib/python3.8/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121a519-0778-428f-b7d1-3db8e55f939b",
   "metadata": {},
   "source": [
    "### VaderSentiment Python Package\n",
    "\n",
    "You can read the orignal paper by authors [here](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52a40258-0402-4723-9cd9-dba70f2b16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d34b3-68bc-4f91-8eca-bd085fe325f4",
   "metadata": {},
   "source": [
    "Define some input texts we going to analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0d002cf-c00d-4d13-a6cf-640576812c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXTS = [{'text': \"This is a bad ass movie. You got to see it! :-)\"},\n",
    "               {'text': \"Ricky Gervais is smart, witty, and creative!!!!!! :D\"},\n",
    "               {'text': \"LOL, this guy fell off a chair while sleeping and snoring in a meeting\"},\n",
    "               {'text': \"Men shoots himself while trying to steal a dog, OMG\"},\n",
    "               {'text': \"Ray and Ray Serve just rocks. Love the way you easily define Ray Actors. Simple APIs, and they work!\"},\n",
    "               {'text': \"Yay!! Another good phone interview. I nailed it!!\"},\n",
    "               {'text': \"This is INSANE! I can't believe it. How could you do such a horrible thing?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130737b3-3907-4e5c-99da-1d19b7f140b9",
   "metadata": {},
   "source": [
    "### Start the Ray Serve. It automatically starts Ray processes on the local host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "502be717-33f6-4a00-abdf-8e6d228d98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=33900)\u001b[0m 2022-01-27 07:49:21,035\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=33900)\u001b[0m 2022-01-27 07:49:21,037\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:wWaVYt:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-01-27 07:49:21,868\tINFO api.py:463 -- Started Serve instance in namespace 'serve'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fc3c86faee0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=35181)\u001b[0m INFO:     Started server process [35181]\n"
     ]
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f828a08-ddc0-4f20-a2dc-77feb33e27bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ray Serve Archictecture and components\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/architecture.svg\" height=\"50%\" width=\"60%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "\n",
    "**Controller**: A global actor unique to each Serve instance that manages the control plane. The Controller is responsible for creating, updating, and destroying other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "**Router**: There is one router per node. Each router is a Uvicorn HTTP server that accepts incoming requests, forwards them to replicas, and responds once they are completed.\n",
    "\n",
    "**Worker Replica**: Worker replicas actually execute the code in response to a request. For example, they may contain an instantiation of an ML model. Each replica processes individual requests from the routers (they may be batched by the replica using `@serve.batch`, see the [batching docs](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d3a8f-2278-4008-8ef6-4f9e16f73036",
   "metadata": {},
   "source": [
    "## Lifetime of a Request¶\n",
    "\n",
    "When an HTTP request is sent to the router, the follow things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6102c-3b6b-401d-a8c6-a7f6fc314a6d",
   "metadata": {},
   "source": [
    "### Define a SocialMediaAnalyserModel\n",
    "\n",
    "This is our main Ray Serve deployment. We can provide route prefix as our path `/sentiments` as part of URL\n",
    "to send requests to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6df2740-2ed5-4e3b-80af-e9fce8396eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/sentiments\")\n",
    "class SocialMediaAnalyserModel(object):\n",
    "    def __init__(self):\n",
    "      \"\"\"\n",
    "      Constructor for our Sentiment Analyser\n",
    "      \"\"\"\n",
    "      # Initialize an instance of vader analyser\n",
    "      self._analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    async def __call__(self, starlette_request):\n",
    "        payload = await starlette_request.json()\n",
    "        text = payload['text']\n",
    "        print(\"Worker: received starlette request with sentimet text\", text)\n",
    "        scores = await self._score(text)\n",
    "        print(f\"<{text}> --> {str(scores)}>\")\n",
    "        \n",
    "    async def _score(self, text):\n",
    "        \"\"\"\n",
    "        Private function to analyse the scores. It invokes model's \n",
    "        param: text to analyse\n",
    "        return: sentiment analyses scores\n",
    "        \"\"\"\n",
    "        scores = self._analyser.polarity_scores(text)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9bd07-dc69-464e-aa71-8f73d69dcf63",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37ab7c38-55f6-4359-ab3a-0bab664a684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 07:49:24,156\tINFO api.py:242 -- Updating deployment 'SocialMediaAnalyserModel'. component=serve deployment=SocialMediaAnalyserModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=33900)\u001b[0m 2022-01-27 07:49:24,256\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'SocialMediaAnalyserModel'. component=serve deployment=SocialMediaAnalyserModel\n",
      "2022-01-27 07:49:25,114\tINFO api.py:249 -- Deployment 'SocialMediaAnalyserModel' is ready at `http://127.0.0.1:8000/sentiments`. component=serve deployment=SocialMediaAnalyserModel\n"
     ]
    }
   ],
   "source": [
    "SocialMediaAnalyserModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eaeed1-0f77-48a7-b349-a481e9bafed0",
   "metadata": {},
   "source": [
    "We can now send HTTP requests to our route `route_prefix=/sentiments` at the default port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a11f2ec3-3558-429c-84a5-47d3b870b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text This is a bad ass movie. You got to see it! :-)\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <This is a bad ass movie. You got to see it! :-)> --> {'neg': 0.0, 'neu': 0.542, 'pos': 0.458, 'compound': 0.7644}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text Ricky Gervais is smart, witty, and creative!!!!!! :D\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <Ricky Gervais is smart, witty, and creative!!!!!! :D> --> {'neg': 0.0, 'neu': 0.316, 'pos': 0.684, 'compound': 0.8957}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text LOL, this guy fell off a chair while sleeping and snoring in a meeting\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <LOL, this guy fell off a chair while sleeping and snoring in a meeting> --> {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.5473}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text Men shoots himself while trying to steal a dog, OMG\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <Men shoots himself while trying to steal a dog, OMG> --> {'neg': 0.262, 'neu': 0.738, 'pos': 0.0, 'compound': -0.4939}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text Ray and Ray Serve just rocks. Love the way you easily define Ray Actors. Simple APIs, and they work!\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <Ray and Ray Serve just rocks. Love the way you easily define Ray Actors. Simple APIs, and they work!> --> {'neg': 0.0, 'neu': 0.712, 'pos': 0.288, 'compound': 0.784}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text Yay!! Another good phone interview. I nailed it!!\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <Yay!! Another good phone interview. I nailed it!!> --> {'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'compound': 0.816}>\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m Worker: received starlette request with sentimet text This is INSANE! I can't believe it. How could you do such a horrible thing?\n",
      "\u001b[2m\u001b[36m(SocialMediaAnalyserModel pid=35183)\u001b[0m <This is INSANE! I can't believe it. How could you do such a horrible thing?> --> {'neg': 0.357, 'neu': 0.643, 'pos': 0.0, 'compound': -0.8034}>\n"
     ]
    }
   ],
   "source": [
    "import requests  # for making web requests\n",
    "for sentiment in INPUT_TEXTS:\n",
    "    response = requests.get(\n",
    "    \"http://localhost:8000/sentiments\", json=sentiment)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276ffb-c7df-4055-9824-a25b2087d084",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "666c7ea6-dde2-4fe2-be14-34d0d82b6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployments: {'SocialMediaAnalyserModel': Deployment(name=SocialMediaAnalyserModel,version=None,route_prefix=/sentiments)}\n"
     ]
    }
   ],
   "source": [
    "deployments = serve.list_deployments()\n",
    "print(f'deployments: {deployments}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5be0ffdb-3fdd-4a2e-90bf-57c9bde3b1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=33900)\u001b[0m 2022-01-27 07:50:01,685\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'SocialMediaAnalyserModel'. component=serve deployment=SocialMediaAnalyserModel\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9199b0-a51a-479f-9896-488dfc28bd00",
   "metadata": {},
   "source": [
    "## Exercise - Try Adding more examples\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Add more neutral, negative and positive texts.\n",
    "2. Add more replicas and view them in the dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
