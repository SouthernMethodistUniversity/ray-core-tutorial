{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f8ad5d-4621-4fa2-887b-9696008d9b42",
   "metadata": {},
   "source": [
    "# Ray crash course - Distributed HPO with Ray Tune and XGBoost-Ray\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "This demo introduces **Ray tune's** key concepts using a classification example. This example is derived from [Hyperparameter Tuning with Ray Tune and XGBoost-Ray](https://github.com/ray-project/xgboost_ray#hyperparameter-tuning). Basically, there are three basic steps or Ray Tune pattern for you as a newcomer to get started with using Ray Tune.\n",
    "\n",
    "Three simple steps:\n",
    "\n",
    " 1. Setup your config space and define your trainable and objective function\n",
    " 2. Use Tune to execute your training hyperparameter sweep, supplying the appropriate arguments including: search space, [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#summary) or [trial schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers)\n",
    " 3. Examine or analyse the results returned\n",
    " \n",
    " <img src=\"https://docs.ray.io/en/latest/_images/tune-workflow.png\" height=\"50%\" width=\"60%\">\n",
    "\n",
    "\n",
    "See also the [Understanding Hyperparameter Tuning](https://github.com/anyscale/academy/blob/main/ray-tune/02-Understanding-Hyperparameter-Tuning.ipynb) notebook and the [Tune documentation](http://tune.io), in particular, the [API reference](https://docs.ray.io/en/latest/tune/api_docs/overview.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b8885d-b698-419f-b43f-d91a073052c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "CONNECT_TO_ANYSCALE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94dd77f0-672e-461c-ad45-ff9b5079c819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAuthenticating\u001b[0m\n",
      "Loaded Anyscale authentication token from ANYSCALE_CLI_TOKEN.\n",
      "\n",
      "\u001b[1m\u001b[36mOutput\u001b[0m\n",
      "\u001b[22m\u001b[33m(anyscale +0.6s)\u001b[0m WARNING: No working_dir specified! Files will only be uploaded to the cluster if a working_dir is provided or a project is detected. In the future, files will only be uploaded if working_dir is provided. To ensure files continue being imported going forward, set the working_dir in your runtime environment. See https://docs.ray.io/en/latest/handling-dependencies.html#runtime-environments.\n",
      "\u001b[1m\u001b[36m(anyscale +0.4s)\u001b[0m Using default project, id: prj_DKZuDR2pUwMzpVZD6PybXaUK.\n",
      "\u001b[1m\u001b[36m(anyscale +0.6s)\u001b[0m cluster jsd-ray-core-tutorial is currently running, the cluster will not be restarted.\n",
      "\u001b[1m\u001b[36m(anyscale +10.4s)\u001b[0m Connected to jsd-ray-core-tutorial, see: https://console.anyscale.com/projects/prj_DKZuDR2pUwMzpVZD6PybXaUK/clusters/ses_DabrzRMt26MfefUwmgcZSxcu\n",
      "\u001b[1m\u001b[36m(anyscale +10.4s)\u001b[0m URL for head node of cluster: https://session-dabrzrmt26mfefuwmgczsxcu.i.anyscaleuserdata.com\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "    if CONNECT_TO_ANYSCALE:\n",
    "        ray.init(\"anyscale://jsd-ray-core-tutorial\")\n",
    "    else:\n",
    "        ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7515b4-c4eb-4068-a908-f53769ef35ab",
   "metadata": {},
   "source": [
    "## Step 1: Define a 'Trainable' training function to use with Ray Tune `ray.tune(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b632024-1e88-4972-8687-46d58a5910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_ACTORS = 4           # degree of parallel trials; each actor will have a separate trial with a set of unique config from the search space\n",
    "NUM_OF_CPUS_PER_ACTOR = 1   # number of CPUs per actor\n",
    "\n",
    "ray_params = RayParams(num_actors=NUM_OF_ACTORS, cpus_per_actor=NUM_OF_CPUS_PER_ACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ab5961-daac-4986-a9d3-5c224990c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_model(config:dict, checkpoint_dir=None):\n",
    "    # create the dataset\n",
    "    train_X, train_y = load_breast_cancer(return_X_y=True)\n",
    "    # Convert to RayDMatrix data structure\n",
    "    train_set = RayDMatrix(train_X, train_y)\n",
    "\n",
    "    # Empty dictionary for the evaluation results reported back\n",
    "    # to tune\n",
    "    evals_result = {}\n",
    "\n",
    "    # Train the model with XGBoost train\n",
    "    bst = train(\n",
    "        params=config,                       # our hyperparameter search space\n",
    "        dtrain=train_set,                    # our RayDMatrix data structure\n",
    "        evals_result=evals_result,           # place holder for results\n",
    "        evals=[(train_set, \"train\")],\n",
    "        verbose_eval=False,\n",
    "        ray_params=ray_params)                # distributed parameters configs for Ray Tune\n",
    "\n",
    "    bst.save_model(\"model.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f68cc5-6999-4bef-99e8-f68c6a5052c5",
   "metadata": {},
   "source": [
    "## Step 2: Define a hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a35e0c3-f0e5-4efd-a7a8-ffdf28dc5216",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Specify the typical hyperparameter search space\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"max_depth\": tune.randint(1, 9)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee450f-0eee-4fa1-ad81-f39a3247ef12",
   "metadata": {},
   "source": [
    "## Step 3: Run Ray tune main trainer and examine the results\n",
    "\n",
    "Ray Tune will launch distributed HPO, using four remote actors, each with its own instance of the trainable func\n",
    "\n",
    "<img src=\"images/ray_tune_dist_hpo.png\" height=\"60%\" width=\"70%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36a1548-90e7-4699-98d1-7e88cbab154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current time: 2022-03-28 14:49:08 (running for 00:00:00.23)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Memory usage on this node: 2.3/61.4 GiB\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Resources requested: 0/80 CPUs, 0/0 GPUs, 0.0/215.68 GiB heap, 0.0/91.5 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Result logdir: /home/ray/ray_results/train_func_model_2022-03-28_14-49-07\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Number of trials: 4/4 (4 PENDING)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func_model pid=950)\u001b[0m 2022-03-28 14:49:10,503\tINFO main.py:985 -- [RayXGBoost] Created 4 new actors (4 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(train_func_model pid=950)\u001b[0m 2022-03-28 14:49:12,522\tINFO main.py:1030 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1029)\u001b[0m [14:49:12] task [xgboost.ray]:140334922639488 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1026)\u001b[0m [14:49:12] task [xgboost.ray]:140259044624512 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1027)\u001b[0m [14:49:12] task [xgboost.ray]:140157224888448 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1028)\u001b[0m [14:49:12] task [xgboost.ray]:139946679531648 got new rank 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current time: 2022-03-28 14:49:18 (running for 00:00:10.30)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Memory usage on this node: 2.9/61.4 GiB\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Resources requested: 20.0/80 CPUs, 0/0 GPUs, 0.0/215.68 GiB heap, 0.0/91.5 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Result logdir: /home/ray/ray_results/train_func_model_2022-03-28_14-49-07\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Number of trials: 4/4 (4 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func_model pid=185, ip=172.31.90.159)\u001b[0m 2022-03-28 14:49:18,374\tINFO main.py:985 -- [RayXGBoost] Created 4 new actors (4 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.89.181)\u001b[0m 2022-03-28 14:49:18,319\tINFO main.py:985 -- [RayXGBoost] Created 4 new actors (4 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.69.42)\u001b[0m 2022-03-28 14:49:18,391\tINFO main.py:985 -- [RayXGBoost] Created 4 new actors (4 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(train_func_model pid=950)\u001b[0m 2022-03-28 14:49:18,471\tINFO main.py:1509 -- [RayXGBoost] Finished XGBoost training on training data with total N=569 in 8.15 seconds (5.94 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current time: 2022-03-28 14:49:19 (running for 00:00:11.51)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Memory usage on this node: 2.9/61.4 GiB\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Resources requested: 15.0/80 CPUs, 0/0 GPUs, 0.0/215.68 GiB heap, 0.0/91.5 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current best trial: e4ca2_00003 with train-error=0.031634 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.00026114429490650996, 'subsample': 0.5976619476448475, 'max_depth': 3, 'nthread': 1, 'n_jobs': 1}\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Result logdir: /home/ray/ray_results/train_func_model_2022-03-28_14-49-07\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.89.181)\u001b[0m 2022-03-28 14:49:20,238\tINFO main.py:1030 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=265, ip=172.31.89.181)\u001b[0m [14:49:20] task [xgboost.ray]:139626710455104 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=268, ip=172.31.89.181)\u001b[0m [14:49:20] task [xgboost.ray]:140345526932288 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=266, ip=172.31.89.181)\u001b[0m [14:49:20] task [xgboost.ray]:139622177989440 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=267, ip=172.31.89.181)\u001b[0m [14:49:20] task [xgboost.ray]:140242534789904 got new rank 2\n",
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.69.42)\u001b[0m 2022-03-28 14:49:20,409\tINFO main.py:1030 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=232, ip=172.31.69.42)\u001b[0m [14:49:20] task [xgboost.ray]:140236976400512 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=237, ip=172.31.69.42)\u001b[0m [14:49:20] task [xgboost.ray]:140368951449728 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=236, ip=172.31.69.42)\u001b[0m [14:49:20] task [xgboost.ray]:140413691824256 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=234, ip=172.31.69.42)\u001b[0m [14:49:20] task [xgboost.ray]:139912696264880 got new rank 1\n",
      "\u001b[2m\u001b[36m(train_func_model pid=185, ip=172.31.90.159)\u001b[0m 2022-03-28 14:49:20,392\tINFO main.py:1030 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=232, ip=172.31.90.159)\u001b[0m [14:49:20] task [xgboost.ray]:139757746223232 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=236, ip=172.31.90.159)\u001b[0m [14:49:20] task [xgboost.ray]:140427209853056 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=235, ip=172.31.90.159)\u001b[0m [14:49:20] task [xgboost.ray]:140178834487424 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=237, ip=172.31.90.159)\u001b[0m [14:49:20] task [xgboost.ray]:140370686003328 got new rank 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:21,346\tINFO commands.py:292 -- Checking External environment settings\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:23,796\tWARN util.py:132 -- The `head_node` field is deprecated and will be ignored. Use `head_node_type` and `available_node_types` instead.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:23,796\tWARN util.py:137 -- The `worker_nodes` field is deprecated and will be ignored. Use `available_node_types` instead.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \u001b[1m\u001b[36mAuthenticating\u001b[0m\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Loaded Anyscale authentication token from variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:25,719\tINFO command_runner.py:357 -- Fetched IP: 172.31.90.159\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:25,719\tINFO log_timer.py:25 -- NodeUpdater: ins_xq9kwXSZLKUYZFz6dfDxT3jD: Got IP  [LogTimer=29ms]\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current time: 2022-03-28 14:49:26 (running for 00:00:18.90)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Memory usage on this node: 2.5/61.4 GiB\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Resources requested: 15.0/80 CPUs, 0/0 GPUs, 0.0/215.68 GiB heap, 0.0/91.5 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current best trial: e4ca2_00003 with train-error=0.031634 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.00026114429490650996, 'subsample': 0.5976619476448475, 'max_depth': 3, 'nthread': 1, 'n_jobs': 1}\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Result logdir: /home/ray/ray_results/train_func_model_2022-03-28_14-49-07\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:26,836\tWARN commands.py:269 -- Loaded cached provider configuration\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:26,836\tWARN commands.py:270 -- If you experience issues with the cloud provider, try re-running the command with --no-config-cache.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:27,828\tINFO command_runner.py:357 -- Fetched IP: 172.31.69.42\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:27,828\tINFO log_timer.py:25 -- NodeUpdater: ins_bY7YuShBYnhFiywtV5MiHjeS: Got IP  [LogTimer=28ms]\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:29,842\tINFO command_runner.py:357 -- Fetched IP: 172.31.89.181\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:29,842\tINFO log_timer.py:25 -- NodeUpdater: ins_nraGV8zWUELca5SzfdiU7fzz: Got IP  [LogTimer=34ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.69.42)\u001b[0m 2022-03-28 14:49:30,770\tINFO main.py:1509 -- [RayXGBoost] Finished XGBoost training on training data with total N=569 in 12.57 seconds (10.36 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(train_func_model pid=185, ip=172.31.90.159)\u001b[0m 2022-03-28 14:49:30,779\tINFO main.py:1509 -- [RayXGBoost] Finished XGBoost training on training data with total N=569 in 12.60 seconds (10.38 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(train_func_model pid=184, ip=172.31.89.181)\u001b[0m 2022-03-28 14:49:30,782\tINFO main.py:1509 -- [RayXGBoost] Finished XGBoost training on training data with total N=569 in 12.67 seconds (10.54 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current time: 2022-03-28 14:49:31 (running for 00:00:23.20)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Memory usage on this node: 2.3/61.4 GiB\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Resources requested: 0/80 CPUs, 0/0 GPUs, 0.0/215.68 GiB heap, 0.0/91.5 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Current best trial: e4ca2_00002 with train-error=0.010545 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0005792447612165456, 'subsample': 0.9104148298038599, 'max_depth': 8, 'nthread': 1, 'n_jobs': 1}\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Result logdir: /home/ray/ray_results/train_func_model_2022-03-28_14-49-07\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m Number of trials: 4/4 (4 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=769)\u001b[0m 2022-03-28 14:49:31,195\tINFO tune.py:639 -- Total run time: 24.22 seconds (23.19 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Run tune\n",
    "analysis = tune.run(\n",
    "    train_func_model,\n",
    "    config=config,\n",
    "    metric=\"train-error\",\n",
    "    mode=\"min\",\n",
    "    num_samples=4,\n",
    "    verbose=1,\n",
    "    resources_per_trial=ray_params.get_tune_resources()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ab3eaa-9002-4950-a29e-9790298de949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0005792447612165456, 'subsample': 0.9104148298038599, 'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters\", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405aff3f-88c9-4c07-b9b1-89dfa6f7a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py:281: UserWarning: Dataframes will use '/' instead of '.' to delimit nested result keys in future versions of Ray. For forward compatibility, set the environment variable TUNE_RESULT_DELIM='/'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss</th>\n",
       "      <th>train-error</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.tree_method</th>\n",
       "      <th>config.objective</th>\n",
       "      <th>config.eval_metric</th>\n",
       "      <th>config.eta</th>\n",
       "      <th>config.subsample</th>\n",
       "      <th>config.max_depth</th>\n",
       "      <th>config.nthread</th>\n",
       "      <th>config.n_jobs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e4ca2_00000</th>\n",
       "      <td>0.370544</td>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0bf6b4af62bb41eea562179a49dabd1c</td>\n",
       "      <td>2022-03-28_14-49-30</td>\n",
       "      <td>1648504170</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0_eta=0.07609,max_depth=1,subsample=0.70435</td>\n",
       "      <td>approx</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>[logloss, error]</td>\n",
       "      <td>0.076090</td>\n",
       "      <td>0.704347</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4ca2_00001</th>\n",
       "      <td>0.686168</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1298d113d7f74c5aa535118099aac140</td>\n",
       "      <td>2022-03-28_14-49-30</td>\n",
       "      <td>1648504170</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1_eta=0.00085375,max_depth=2,subsample=0.99038</td>\n",
       "      <td>approx</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>[logloss, error]</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.990381</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4ca2_00002</th>\n",
       "      <td>0.688075</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>ef3e8d20fb7f4365bbd6fa24932ddbab</td>\n",
       "      <td>2022-03-28_14-49-30</td>\n",
       "      <td>1648504170</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2_eta=0.00057924,max_depth=8,subsample=0.91041</td>\n",
       "      <td>approx</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>[logloss, error]</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.910415</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4ca2_00003</th>\n",
       "      <td>0.690972</td>\n",
       "      <td>0.031634</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>962043591a5047cc85260daedcd74af3</td>\n",
       "      <td>2022-03-28_14-49-18</td>\n",
       "      <td>1648504158</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3_eta=0.00026114,max_depth=3,subsample=0.59766</td>\n",
       "      <td>approx</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>[logloss, error]</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.597662</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             train-logloss  train-error  time_this_iter_s  done  \\\n",
       "trial_id                                                          \n",
       "e4ca2_00000       0.370544     0.056239          0.004565  True   \n",
       "e4ca2_00001       0.686168     0.035149          0.004040  True   \n",
       "e4ca2_00002       0.688075     0.010545          0.005621  True   \n",
       "e4ca2_00003       0.690972     0.031634          0.004894  True   \n",
       "\n",
       "            timesteps_total episodes_total  training_iteration  \\\n",
       "trial_id                                                         \n",
       "e4ca2_00000            None           None                  10   \n",
       "e4ca2_00001            None           None                  10   \n",
       "e4ca2_00002            None           None                  10   \n",
       "e4ca2_00003            None           None                  10   \n",
       "\n",
       "                                experiment_id                 date  \\\n",
       "trial_id                                                             \n",
       "e4ca2_00000  0bf6b4af62bb41eea562179a49dabd1c  2022-03-28_14-49-30   \n",
       "e4ca2_00001  1298d113d7f74c5aa535118099aac140  2022-03-28_14-49-30   \n",
       "e4ca2_00002  ef3e8d20fb7f4365bbd6fa24932ddbab  2022-03-28_14-49-30   \n",
       "e4ca2_00003  962043591a5047cc85260daedcd74af3  2022-03-28_14-49-18   \n",
       "\n",
       "              timestamp  ...  iterations_since_restore  \\\n",
       "trial_id                 ...                             \n",
       "e4ca2_00000  1648504170  ...                        10   \n",
       "e4ca2_00001  1648504170  ...                        10   \n",
       "e4ca2_00002  1648504170  ...                        10   \n",
       "e4ca2_00003  1648504158  ...                        10   \n",
       "\n",
       "                                             experiment_tag  \\\n",
       "trial_id                                                      \n",
       "e4ca2_00000     0_eta=0.07609,max_depth=1,subsample=0.70435   \n",
       "e4ca2_00001  1_eta=0.00085375,max_depth=2,subsample=0.99038   \n",
       "e4ca2_00002  2_eta=0.00057924,max_depth=8,subsample=0.91041   \n",
       "e4ca2_00003  3_eta=0.00026114,max_depth=3,subsample=0.59766   \n",
       "\n",
       "            config.tree_method config.objective  config.eval_metric  \\\n",
       "trial_id                                                              \n",
       "e4ca2_00000             approx  binary:logistic    [logloss, error]   \n",
       "e4ca2_00001             approx  binary:logistic    [logloss, error]   \n",
       "e4ca2_00002             approx  binary:logistic    [logloss, error]   \n",
       "e4ca2_00003             approx  binary:logistic    [logloss, error]   \n",
       "\n",
       "             config.eta  config.subsample config.max_depth config.nthread  \\\n",
       "trial_id                                                                    \n",
       "e4ca2_00000    0.076090          0.704347                1              1   \n",
       "e4ca2_00001    0.000854          0.990381                2              1   \n",
       "e4ca2_00002    0.000579          0.910415                8              1   \n",
       "e4ca2_00003    0.000261          0.597662                3              1   \n",
       "\n",
       "            config.n_jobs  \n",
       "trial_id                   \n",
       "e4ca2_00000             1  \n",
       "e4ca2_00001             1  \n",
       "e4ca2_00002             1  \n",
       "e4ca2_00003             1  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.results_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7f8e5-bf4c-436f-95b1-8609aecf170f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6673aba4-4d53-4c86-8cf2-e82e3a035aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1886c8e-70ba-4776-94cd-a12916732f67",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * [Ray Train: Tune: Scalable Hyperparameter Tuning](https://docs.ray.io/en/master/tune/index.html)\n",
    " * [Introducing Distributed XGBoost Training with Ray](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)\n",
    " * [How to Speed Up XGBoost Model Training](https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training)\n",
    " * [XGBoost-Ray Project](https://github.com/ray-project/xgboost_ray)\n",
    " * [Distributed XGBoost on Ray](https://docs.ray.io/en/latest/xgboost-ray.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
