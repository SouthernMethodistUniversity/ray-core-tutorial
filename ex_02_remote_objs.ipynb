{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of Ray Core: Remote Objects\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_03_remote_classes.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_01_remote_funcs.ipynb) <br>\n",
    "\n",
    "### Overview\n",
    "\n",
    "\n",
    "In Ray, tasks and actors create and compute on objects. We refer to these objects as remote objects because they can be stored anywhere in a Ray cluster, and we use object refs to refer to them. Remote objects are cached in Ray‚Äôs distributed shared-memory object store, and there is one object store per node in the cluster. In the cluster setting, a remote object can live on one or many nodes, independent of who holds the object ref(s).\n",
    "\n",
    "[*Remote Objects*](https://docs.ray.io/en/latest/walkthrough.html#objects-in-ray)\n",
    "reside in a distributed [*shared-memory object store*](https://en.wikipedia.org/wiki/Shared_memory).\n",
    "\n",
    "Objects are immutable and can be accessed from anywhere on the cluster, as they are stored in the cluster shared memory. An object ref is essentially a pointer or a unique ID that can be used to refer to a remote object without seeing its value. If you‚Äôre familiar with futures in Python, Java or Scala, Ray object refs are conceptually similar.\n",
    "\n",
    "\n",
    "In general, small objects are stored in their owner‚Äôs **in-process store** (**<=100KB**), while large objects are stored in the **distributed object store**. This decision is meant to reduce the memory footprint and resolution time for each object. Note that in the latter case, a placeholder object is stored in the in-process store to indicate the object has been promoted to shared memory.\n",
    "\n",
    "In the case if there is no space in the shared-memory, objects are spilled over to disk. But the main point here is that\n",
    "shared-memory allows _zero-copy_ access to processes on the same worker node.\n",
    "\n",
    "<img src=\"images/shared_memory_plasma_store.png\"  height=\"40%\" width=\"65%\">\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial, you learn about:\n",
    " * Ray Futures as one of the patterns\n",
    " * Ray's distributed Plasma object store\n",
    " * How obejcts are stored and fetched from the distributed shared object store\n",
    "     * Use `ray.get` and `ray.put` examples\n",
    " * How to use tasks and object store to train and tune regression model: sequential vs. parallel\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Object references as futures pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.1</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='2.0.1', ray_commit='03b6bc7b5a305877501110ec04710a9c57011479', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-11-19_13-33-08_126954_95021/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-11-19_13-33-08_126954_95021/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-11-19_13-33-08_126954_95021', 'metrics_export_port': 58829, 'gcs_address': '127.0.0.1:61721', 'address': '127.0.0.1:61721', 'dashboard_agent_listen_port': 52365, 'node_id': '34250f1500560d9e1d15806f5c505ed01ea7a286d06fb84995389d33'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Objects example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll create some python objects and put them in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list = [23, 42, 93]\n",
    "\n",
    "# returns an objectRef\n",
    "obj_ref = ray.put(num_list)\n",
    "obj_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then retrieve the value of this object reference. \n",
    "\n",
    "Small objects are resolved by copying them directly from the _owner‚Äôs_ **in-process store**. For example, if the owner calls `ray.get`, the system looks up and deserializes the value from the local **in-process store**. For larger objects greater than 100KB, they will be stored in the distributed object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 42, 93]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = ray.get(obj_ref)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can gather the values of multiple object references in parallel using a list comprehension:\n",
    " 1. Each value is put in the object store and its `ObjRefID` is immediately returned\n",
    " 2. The comprehension constructs a list of `ObjRefIDs` for each element in the loop\n",
    " 3. A final `get(list_obj_refs`) is invoked to fetch the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ray.get([ray.put(i) for i in range(10)])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Objects by Reference\n",
    "\n",
    "Ray object references can be freely passed around a Ray application. This means that they can be passed as arguments to tasks, actor methods, and even stored in other objects. Objects are tracked via distributed reference counting, and their data is automatically freed once all references to the object are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Task\n",
    "@ray.remote\n",
    "def echo(x):\n",
    "    print(f\"current value of argument x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "x = list(range(10))\n",
    "obj_ref_x = ray.put(x)\n",
    "y = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass-by-value\n",
    "\n",
    "Send the object to a task as a top-level argument.\n",
    "The object will be *de-referenced* automatically, so the task only sees its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(echo pid=96385)\u001b[0m current value of argument x: 25\n"
     ]
    }
   ],
   "source": [
    "# send y as value argument\n",
    "echo.remote(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(16310a0f0a45af5cffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(echo pid=96385)\u001b[0m current value of argument x: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# send a an object reference\n",
    "# note that the echo function deferences it\n",
    "echo.remote(obj_ref_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass-by-reference\n",
    "\n",
    "When a parameter is passed inside a Python list or as any other data structure,\n",
    "the *object ref is preserved*, meaning it's not *de-referenced*. The object data is not transferred to the worker when it is passed by reference, until `ray.get()` is called on the reference.\n",
    "\n",
    "You can pass by reference in two ways:\n",
    " 1. as a dictionary `.remote({\"obj\": obj_ref_x})`\n",
    " 2. as list of objRefs `.remote([obj_ref_x])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(c2668a65bda616c1ffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(echo pid=96385)\u001b[0m current value of argument x: {'obj': ObjectRef(00ffffffffffffffffffffffffffffffffffffff010000000d000000)}\n"
     ]
    }
   ],
   "source": [
    "x = list(range(20))\n",
    "obj_ref_x = ray.put(x)\n",
    "# Echo will not automaticall de-reference it\n",
    "echo.remote({\"obj\": obj_ref_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(32d950ec0ccf9d2affffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(echo pid=96385)\u001b[0m current value of argument x: [ObjectRef(00ffffffffffffffffffffffffffffffffffffff010000000d000000)]\n"
     ]
    }
   ],
   "source": [
    "echo.remote([obj_ref_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about long running tasks?\n",
    "\n",
    "Sometimes, you may have tasks that are long running, past their expected times due to some problem, maybe blocked on accessing a variable in the object store. How do you exit or terminate it? Use a timeout!\n",
    "\n",
    "Now let's set a timeout to return early from an attempted access of a remote object that is blocking for too long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@ray.remote\n",
    "def long_running_function ():\n",
    "    time.sleep(10)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control how long you want to wait for the task to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`get` timed out\n",
      "CPU times: user 32.2 ms, sys: 19.6 ms, total: 51.8 ms\n",
      "Wall time: 6.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from ray.exceptions import GetTimeoutError\n",
    "\n",
    "obj_ref = long_running_function.remote()\n",
    "\n",
    "try:\n",
    "    ray.get(obj_ref, timeout=6)\n",
    "except GetTimeoutError:\n",
    "    print(\"`get` timed out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Hands-on code example - scaling regression \n",
    "Use Ray Tasks and Object store\n",
    "\n",
    "In this example, you will run an illustrative code example that will give you better \"feel\" of Ray. Specifically, you will use Ray Core tasks and object store to scale a bare bones version of a common ML task: regression on the structured data.\n",
    "\n",
    "#### Data\n",
    "\n",
    "Dataset is [California House Prices](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) as available via scikit-learn.\n",
    "\n",
    "|<img src=\"images/California_dataset.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|`n_samples = 20640`, target is numeric and corresponds to the average house value in units of 100k.|\n",
    "\n",
    "#### Model and task\n",
    "\n",
    "You will train and score [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) models using [mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) metric.\n",
    "\n",
    "In order to find the best performing model you will train many models with varying `n_estimators` hyper-parameter. This brings the topic of sequential vs. parallel model training. You will first first implement the sequential approach, then improve it by distributing training with Ray Core - you will achieve better performance and faster model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential implementation\n",
    "\n",
    "Vanilla implementation assumes sequential training. Models are trained one by one in the sequential way, as depicted on the diagram below. \n",
    "\n",
    "|<img src=\"images/sequential_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential tasks, one after the other.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 10\n",
    "NUM_ESTIMATORS_START = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function to train and score model\n",
    "\n",
    "This function takes data, instantiates `RandomForestRegressor` model, trains it and scores the model on the test set.\n",
    "\n",
    "Function returns tuple:\n",
    "```\n",
    "(n_estimators, mse_score)\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "(100, 0.2596393978947323)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    n_estimators: int,\n",
    "):\n",
    "    start_time = time.time()  # measure wall time for single model training\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    time_delta = time.time() - start_time\n",
    "    print(f\"n_estimators={n_estimators}, mse={score:.4f}, took: {time_delta:.2f} seconds\")\n",
    "\n",
    "    return n_estimators, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function that runs **sequential** model training\n",
    "This function train `n_models` sequentially for the increasing number of `n_estimators` (it increases by 5 for each model, so 100, 105, 110, 115, ...). \n",
    "\n",
    "Function returns list of tuples:\n",
    "```\n",
    "[(n_estimators, mse_score), (n_estimators, mse_score), ...]\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "[(100, 0.2596393978947323), (105, 0.26009608813335056), (110, 0.25980497900048843), (115, 0.2595780807428954), (120, 0.2600438515882204)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential(n_models: int):\n",
    "    # Use comprehension to run n_models in a sequential manner\n",
    "    return [\n",
    "        train_and_score_model(\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            n_estimators=NUM_ESTIMATORS_START + 5 * j,\n",
    "        )\n",
    "        for j in range(n_models)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run sequential model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100, mse=0.2554, took: 6.48 seconds\n",
      "n_estimators=105, mse=0.2558, took: 6.77 seconds\n",
      "n_estimators=110, mse=0.2556, took: 7.09 seconds\n",
      "n_estimators=115, mse=0.2556, took: 7.41 seconds\n",
      "n_estimators=120, mse=0.2551, took: 7.74 seconds\n",
      "n_estimators=125, mse=0.2551, took: 8.06 seconds\n",
      "n_estimators=130, mse=0.2548, took: 8.38 seconds\n",
      "n_estimators=135, mse=0.2550, took: 8.72 seconds\n",
      "n_estimators=140, mse=0.2550, took: 9.06 seconds\n",
      "n_estimators=145, mse=0.2549, took: 9.36 seconds\n",
      "CPU times: user 1min 18s, sys: 437 ms, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mse_scores = run_sequential(n_models=NUM_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: ~80 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: mse=0.2548, n_estimators=130\n"
     ]
    }
   ],
   "source": [
    "best = min(mse_scores, key=itemgetter(1))\n",
    "print(f\"Best model: mse={best[1]:.4f}, n_estimators={best[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training completed, but performance is slow due to sequential nature of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel implementation\n",
    "\n",
    "Now, you use Ray to train these models in parallel, utilizing all available resources. Diagram below gives visual intuition for this setup.\n",
    "\n",
    "|<img src=\"images/distributed_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Sample timeline with ten tasks running across 4 worker nodes in parallel with minor overhead from scheduler.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put data in the object store\n",
    "\n",
    "This allows all training tasks to fetch data object the shared memory object store.\n",
    "\n",
    "Your data is now available for all remote Tasks and Actors in the cluster. You use Object ID to reference the object when needed.\n",
    "\n",
    "Example Object ID look like this:\n",
    "\n",
    "ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000002000000)\n",
    "\n",
    "|<img src=\"images/x_y_training_data.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "\n",
    "Training and test data inserted into the shared memory of the nodes \n",
    "\n",
    "Two presentations and blogs that show case how to reduce latency and load times using\n",
    "object store are worthy reads\n",
    "\n",
    " * [How to Load PyTorch Models 340 Times Faster with Ray](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c)\n",
    " * [Zero-copy model loading with Ray and PyTorch](https://www.anyscale.com/ray-summit-2022/agenda/sessions/172)\n",
    " * [Data transfer speed comparison in a distributed ML application: Ray Plasma store vs. S3](https://www.anyscale.com/events/2022/09/29/ray-meetup-community-talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ref = ray.put(X_train)\n",
    "X_test_ref = ray.put(X_test)\n",
    "y_train_ref = ray.put(y_train)\n",
    "y_test_ref = ray.put(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function as a Ray task to train and score model\n",
    "\n",
    "You simple decorate the function with `ray.remote`. Note that the function is similar to one above.\n",
    "This Ray task will run on a Ray cluster node's worker process.\n",
    "\n",
    "* It is exactly the same function as in the sequential example\n",
    "* You added `@ray.remote` decorator to specify that this function will be executed as a remote task in a different process (remotely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_and_score_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    n_estimators: int,\n",
    "):\n",
    "    start_time = time.time()  # measure wall time for single model training\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=201)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    time_delta = time.time() - start_time\n",
    "    print(f\"n_estimators={n_estimators}, mse={score:.4f}, took: {time_delta:.2f} seconds\")\n",
    "\n",
    "    return n_estimators, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function that runs **parallel** model training\n",
    "\n",
    "You modified `run_sequential()` function to achieve parallel execution.\n",
    "\n",
    "**Remote Tasks**\n",
    "\n",
    "* Functions with `.remote` (as in the code above) suffix returns an `ObjectRef` associated with the computations to be done.\n",
    "* When you run a remote function (Ray Task), it will immediately return an `ObjectRef` (Object Reference). It is a *promise* of future work (Python futures), meaning that the task is delegated to a worker, and `ObjectRef` is returned while the task executes in the background. This is an asynchronous operation.\n",
    "* To access the expected output, you call `ray.get()` (as in the code above) on the `ObjectRef` or list of `ObjectRef`. It is a synchronous operation (blocking call). In other words: Use `ray.get()` on the returned list of `ObjectRef` to get remote objects from the object store.\n",
    "\n",
    "Operation:\n",
    "\n",
    "```\n",
    "ray.get([ObjectRef, ObjectRef, ObjectRef, ...])\n",
    "```\n",
    "\n",
    "returns list of `(n_estimators, score)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(n_models: int):\n",
    "    return ray.get(\n",
    "        [\n",
    "            train_and_score_model.remote(\n",
    "                X_train=X_train_ref,\n",
    "                X_test=X_test_ref,\n",
    "                y_train=y_train_ref,\n",
    "                y_test=y_test_ref,\n",
    "                n_estimators=100 + 5 * j,\n",
    "            )\n",
    "            for j in range(n_models)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallel model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_and_score_model pid=96385)\u001b[0m n_estimators=100, mse=0.2548, took: 7.57 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96380)\u001b[0m n_estimators=105, mse=0.2549, took: 7.88 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96382)\u001b[0m n_estimators=110, mse=0.2545, took: 8.19 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96386)\u001b[0m n_estimators=115, mse=0.2542, took: 8.57 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96389)\u001b[0m n_estimators=120, mse=0.2540, took: 8.95 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96383)\u001b[0m n_estimators=125, mse=0.2538, took: 9.17 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96388)\u001b[0m n_estimators=130, mse=0.2538, took: 9.71 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96384)\u001b[0m n_estimators=135, mse=0.2536, took: 9.92 seconds\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96387)\u001b[0m n_estimators=140, mse=0.2534, took: 10.18 seconds\n",
      "CPU times: user 48.3 ms, sys: 31.8 ms, total: 80.1 ms\n",
      "Wall time: 11.2 s\n",
      "\u001b[2m\u001b[36m(train_and_score_model pid=96381)\u001b[0m n_estimators=145, mse=0.2532, took: 10.63 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mse_scores = run_parallel(n_models=NUM_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice **11x performance gain**\n",
    "\n",
    "* Parallel: 11s.\n",
    "* Sequential: 1min 19s (~80s).\n",
    "\n",
    "\n",
    "*(experiment on the M1 MacBook Pro)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: mse=0.2532, n_estimators=145\n"
     ]
    }
   ],
   "source": [
    "best = min(mse_scores, key=itemgetter(1))\n",
    "print(f\"Best model: mse={best[1]:.4f}, n_estimators={best[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Create a list of object references containing integers returned by `ray.put(x)` \n",
    " * Use comprehension to construct this list \n",
    " * write a Ray task, `my_function.remote(list_of_object_refs)`, and return the sum of the list.\n",
    "  * Use `ray.get(...)` to print the sum \n",
    "2. Create large lists and python dictionaries, put them in object store\n",
    "  * Write a Ray task to process them.\n",
    "3. Change the number of estimators and models and run both sequential and parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Read references to get advanced deep dives and more about Ray objects\n",
    "2. [Serialization](https://docs.ray.io/en/latest/ray-core/objects/serialization.html)\n",
    "3. [Memory Management](https://docs.ray.io/en/latest/ray-core/objects/memory-management.html)\n",
    "4. [Object Spilling](https://docs.ray.io/en/latest/ray-core/objects/object-spilling.html)\n",
    "5. [Fault Tolerance](https://docs.ray.io/en/latest/ray-core/objects/fault-tolerance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "We covered how to use Ray `tasks`, `ray.get()` and `ray.put`, understand distributed remote object store, let's move on to the [Ray Actors lesson](ex_03_remote_classes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * [Ray Architecture Reference](https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#)\n",
    " * [Ray Internals: A peek at ray,get](https://www.youtube.com/watch?v=a1kNnQu6vGw)\n",
    " * [Ray Internals: Object management with Ownership Model](https://www.youtube.com/watch?v=1oSBxTayfJc)\n",
    " * [Deep Dive into Ray scheduling Policies](https://www.youtube.com/watch?v=EJUYKXWGzfI)\n",
    " * [Redis in Ray: Past and future](https://www.anyscale.com/blog/redis-in-ray-past-and-future)\n",
    " * [StackOverFlow: How Ray Shares Data](https://stackoverflow.com/questions/58082023/how-exactly-does-ray-share-data-to-workers/71500979#71500979)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_03_remote_classes.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_01_remote_funcs.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
