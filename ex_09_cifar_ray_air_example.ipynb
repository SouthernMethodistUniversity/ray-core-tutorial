{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray AI Runtime: An end-to-end for image classification example\n",
    "\n",
    "[Ray AI Runtime (AIR)](https://docs.ray.io/en/latest/ray-air/getting-started.html) is a scalable and unified toolkit for machine learning (ML) applications. AIR enables simple scaling of individual workloads and end-to-end workflows using popular ML frameworks, all in just Python.\n",
    "\n",
    "AIR builds on Rayâ€™s best-in-class libraries for [Preprocessing](https://docs.ray.io/en/latest/data/dataset.html#datasets), [Training](https://docs.ray.io/en/latest/train/train.html#train-docs), [Tuning](https://docs.ray.io/en/latest/tune/index.html#tune-main), [Scoring](https://docs.ray.io/en/latest/ray-air/predictors.html#air-predictors), [Serving](https://docs.ray.io/en/latest/serve/index.html#rayserve), and [Reinforcement Learning](https://docs.ray.io/en/latest/rllib/index.html#rllib-index) to bring together an ecosystem of integrations. \n",
    "\n",
    "Out of the box with Ray AIR, you can easily build the common machine line pipeline with AIR's components.\n",
    "\n",
    "<img src=\"images/air_ml_workflow.png\" width=\"60%\" height=\"30%\">\n",
    "\n",
    "This notebook employs each of these components to illustrate how in six distinct steps, using each of the Ray AIR [components and its APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray-air-api), to build a common ML pipeline for an image classification.\n",
    "\n",
    "\n",
    "### The CIFAR-10 dataset for classification\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "<img src=\"images/cifar-10.png\" width=\"60%\" height=\"30%\">\n",
    "\n",
    "[Cifar-10 source](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "  * Use AIR's PyTorch Trainers, Tuners, Checkpoint, Batch & Online inference predictors\n",
    "  * Understand how Ray data integrates with Ray AIR for data ingestion and prediction\n",
    "  * Use out-of-box Preprocessors\n",
    "  * Load model from the best model checkpoint and use for batch inference\n",
    "  * Deploy best checkpoint model and use for online inference\n",
    "  * Using all the Ray discrete components to write an end-to-end ML application in a single Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    " \n",
    "import cifar_utils\n",
    "\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune import Tuner\n",
    "from ray.tune import TuneConfig\n",
    "from ray.air.config import RunConfig\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.air.config import CheckpointConfig\n",
    "\n",
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:41:51,018\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.13', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-01-02_13-41-49_000727_25257/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-01-02_13-41-49_000727_25257/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2023-01-02_13-41-49_000727_25257', 'metrics_export_port': 61050, 'gcs_address': '127.0.0.1:65168', 'address': '127.0.0.1:65168', 'dashboard_agent_listen_port': 52365, 'node_id': '2d01d00504a8d7519a45b0d62a37fb28d692e72b66aa4493654c50aa'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': 48791280026.0,\n",
       " 'CPU': 10.0,\n",
       " 'object_store_memory': 2147483648.0,\n",
       " 'node:127.0.0.1': 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Fetch Data\n",
    "\n",
    "Use Ray data to fetch data from a data source. Here we use a simple built-in `SimpleTorchDatasouce`. For preprocessing needs, such as scaling or normalizing, \n",
    "Ray AIR provides a host of preprocessors you can use; these [preprocessors](https://docs.ray.io/en/latest/ray-air/preprocessors.html) can be supplied downstream to `Trainers` and `BatchPredictors` to preprocess your \n",
    "input before training, tuning, or scoring, relieving you the onus of doing it yourself. \n",
    "\n",
    "<img src=\"images/data_prep.png\" width=\"360%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_torchvision_dataset = cifar_utils.train_dataset_factory()\n",
    "test_torchvision_dataset = cifar_utils.test_dataset_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: /Users/jules/data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "            ),\n",
       " torchvision.datasets.cifar.CIFAR10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_torchvision_dataset, type(train_torchvision_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above are PyTorch torchvision data types. We want to convert\n",
    "them into Ray Dataset types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a preprocessor to transform data\n",
    "* Convert Torchvisiion data into Ray data\n",
    "* Transform data into Pandas DataFrame because internally Ray data uses Pandas by default as it internal representation\n",
    "* Use dataset `map_batches`() to convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset(num_blocks=200, num_rows=50000, schema=<class 'tuple'>),\n",
       " ray.data.dataset.Dataset)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset: ray.data.Dataset = ray.data.from_torch(train_torchvision_dataset)\n",
    "test_dataset: ray.data.Dataset = ray.data.from_torch(test_torchvision_dataset)\n",
    "train_dataset, type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 114.18it/s]\n",
      "Map_Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 658.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset(num_blocks=200, num_rows=50000, schema={image: ArrowTensorType(shape=(3, 32, 32), dtype=float), label: int64}),\n",
       " ray.data.dataset.Dataset)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map_batches(cifar_utils.convert_batch_to_numpy)\n",
    "test_dataset = test_dataset.map_batches(cifar_utils.convert_batch_to_numpy)\n",
    "train_dataset, type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a `TorchTrainer` for PyTorch DDP training\n",
    "This trainer is then fed into tuner for doing some hyper-parameter optimization. Ray AIR supports popular [Trainers](https://docs.ray.io/en/latest/ray-air/trainer.html)\n",
    "for the common ML libraries: PyTorch, TensorFlow, XGBoost, scikit-learn. These can be used alongside [Tuner](https://docs.ray.io/en/latest/ray-air/tuner.html)\n",
    "\n",
    "<img src=\"images/trainer.png\" width=\"60%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(train_loop_per_worker=cifar_utils.train_loop_per_worker,\n",
    "                        datasets={\"train\": train_dataset},\n",
    "                        train_loop_config={\"epochs\": 40, \"batch_size\": 32},\n",
    "                        scaling_config=ScalingConfig(num_workers=4, use_gpu=False), # change to True if on GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Tuner \n",
    "\n",
    "Tuning a model over a hyperparameters space is a common workload and paramount to get better accuracy. Here we tune the relevant tune configuration such as `lr`, `batch_size`, `epochs`.\n",
    "However, you can use SOTA [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html) and [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html) alongside [Tuner](https://docs.ray.io/en/latest/tune/api_docs/execution.html#ray.tune.Tuner) to control your trials, depending on your model requirements and desired results.\n",
    "\n",
    "\n",
    "<img src=\"images/tuner.png\" width=\"60%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer,\n",
    "        param_space={\n",
    "            \"train_loop_config\": {\n",
    "                \"lr\": ray.tune.grid_search([0.001, 0.01]),\n",
    "            }\n",
    "        },\n",
    "        # specific tune metrics to collect and checkpoint\n",
    "        # during trials\n",
    "        tune_config=TuneConfig(metric=\"train_loss\", mode=\"min\"),\n",
    "        run_config=RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=1, \n",
    "                        checkpoint_score_attribute=\"train_loss\", \n",
    "                        checkpoint_score_order=\"min\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have set up our Trainer and Tuner, all we need to create distributed training and trials is use the \n",
    "familiar API .`.fit`() to launch our distributed trainiing and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-03 07:09:15</td></tr>\n",
       "<tr><td>Running for: </td><td>17:22:15.19        </td></tr>\n",
       "<tr><td>Memory:      </td><td>34.0/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 10.0/10 CPUs, 0/0 GPUs, 0.0/45.44 GiB heap, 0.0/2.0 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  train_loop_config/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _time_this_iter_s</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_fc79e_00000</td><td>RUNNING </td><td>127.0.0.1:32555</td><td style=\"text-align: right;\">                 0.001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         62146.2</td><td style=\"text-align: right;\">    1.26575 </td><td style=\"text-align: right;\">  1672758168</td><td style=\"text-align: right;\">            3445.67</td></tr>\n",
       "<tr><td>TorchTrainer_fc79e_00001</td><td>RUNNING </td><td>127.0.0.1:32562</td><td style=\"text-align: right;\">                 0.01 </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         62131.8</td><td style=\"text-align: right;\">    0.434617</td><td style=\"text-align: right;\">  1672758156</td><td style=\"text-align: right;\">            3444.93</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32563)\u001b[0m 2023-01-02 13:47:04,260\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32564)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32566)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32563)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32565)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32563)\u001b[0m 2023-01-02 13:47:06,061\tINFO train_loop_utils.py:270 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32563)\u001b[0m 2023-01-02 13:47:06,062\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32581)\u001b[0m 2023-01-02 13:47:07,010\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32584)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32582)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32583)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32581)\u001b[0m [W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32581)\u001b[0m 2023-01-02 13:47:07,643\tINFO train_loop_utils.py:270 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=32581)\u001b[0m 2023-01-02 13:47:07,643\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname             </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_fc79e_00000</td><td style=\"text-align: right;\">            3445.67</td><td style=\"text-align: right;\">  1672758168</td><td style=\"text-align: right;\">                   17</td><td>2023-01-03_07-02-48</td><td>False </td><td>                </td><td>2703a3aeebc241af871d20e41999bab9</td><td>Juless-MacBook-Pro-16</td><td style=\"text-align: right;\">                        17</td><td>127.0.0.1</td><td style=\"text-align: right;\">32555</td><td>True               </td><td style=\"text-align: right;\">             62146.2</td><td style=\"text-align: right;\">           3445.73</td><td style=\"text-align: right;\">       62146.2</td><td style=\"text-align: right;\"> 1672758168</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    1.26575 </td><td style=\"text-align: right;\">                  17</td><td>fc79e_00000</td><td style=\"text-align: right;\">     0.311514</td></tr>\n",
       "<tr><td>TorchTrainer_fc79e_00001</td><td style=\"text-align: right;\">            3444.93</td><td style=\"text-align: right;\">  1672758156</td><td style=\"text-align: right;\">                   17</td><td>2023-01-03_07-02-36</td><td>False </td><td>                </td><td>329889f7d4b44059862c5edb36a3d610</td><td>Juless-MacBook-Pro-16</td><td style=\"text-align: right;\">                        17</td><td>127.0.0.1</td><td style=\"text-align: right;\">32562</td><td>True               </td><td style=\"text-align: right;\">             62131.8</td><td style=\"text-align: right;\">           3444.96</td><td style=\"text-align: right;\">       62131.8</td><td style=\"text-align: right;\"> 1672758156</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">    0.434617</td><td style=\"text-align: right;\">                  17</td><td>fc79e_00001</td><td style=\"text-align: right;\">     0.30891 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the tuner, which will call trainer for each trial with\n",
    "# the parameters configurations as part of it HPO and trains it in parallel\n",
    "# with a shard of its own data. This is distributed training and tunning in parallel\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best checkpoint result\n",
    "best_checkpoint = results.get_best_result(metric=\"train_loss\", mode=\"min\").checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint.to_directory(\"./best_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use `TorchPredictor` using the `Checkpoint` object\n",
    "This allows batch inference at scale:\n",
    "* Test our model with [`TorchPredictor`](https://docs.ray.io/en/latest/_modules/ray/train/torch/torch_predictor.html?highlight=TorchPredictor) using the [`Checkpoint`](https://docs.ray.io/en/latest/ray-air/key-concepts.html#checkpoints) object.\n",
    "* Fetch the best_checkpoint from the  checkpoint. \n",
    "* Use batch predictor to test the entire batch in one go\n",
    "\n",
    "\n",
    "<img src=\"images/batch_predictor.png\" width=\"60%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the label column\n",
    "predict_dataset = test_dataset.drop_columns(cols=[\"label\"])\n",
    "\n",
    "# Create our BatchPredictor from the best checkpoint obtained above after all the trials are finished\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=best_checkpoint,\n",
    "    predictor_cls=TorchPredictor,\n",
    "    model=my_utils.ResNet18(),\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# Do prediction at scale over the entire batch\n",
    "output: ray.data.Dataset = batch_predictor.predict(\n",
    "    data=test_dataset, dtype=torch.float, \n",
    "    feature_columns=[\"image\"], \n",
    "    keep_columns=[\"label\"],\n",
    "    num_gpus_per_worker=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from the output and convert them into classes\n",
    "predictions = output.map_batches(my_utils.convert_logits_to_classes, batch_format=\"pandas\")\n",
    "\n",
    "# Get all predictions for test_dataset \n",
    "scores = predictions.map_batches(my_utils.calculate_prediction_scores)\n",
    "\n",
    "# compute total prediction accuracy. That is all predictions equal to ground truth\n",
    "# That is, predictated accurately.\n",
    "total_acc = scores.sum(on=\"correct\") / scores.count()\n",
    "\n",
    "print(f\"Prediction accuracy from the test data of 10,000 images: {total_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Online prediction\n",
    " * Deploy our model to the network for online prediction\n",
    " * Use `TorchPredictor` and [`PredictorDeployment`](https://docs.ray.io/en/latest/ray-air/package-ref.html?highlight=PredictorDeployment#ray.serve.air_integrations.PredictorDeployment) APIs\n",
    " * Start a Ray serve in detached mode, with two replicas\n",
    " \n",
    " This will automatically deploy the model from the lastest checkpoint. You can adjust num of replicas in the options.\n",
    " \n",
    " <img src=\"images/online_predictor.png\" width=\"60%\" height=\"25%\">\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ray Serve in detached mode so that if this application terminates, Serve can still process the incoming\n",
    "# requests. \n",
    "serve.start(detached=True)\n",
    "\n",
    "# create the PredictorDeployment and give it a name and number of replicas. Depending on your\n",
    "# request loads, you can scale up the number of replicas\n",
    "deployment = PredictorDeployment.options(name=\"cifar-deployment\", num_replicas=2)\n",
    "deployment.deploy(TorchPredictor, best_checkpoint, batching_params=False, model=my_utils.ResNet18(), http_adapter=my_utils.json_to_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test online prediction from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test online deployment\n",
    "batch = test_dataset.take(10)\n",
    "for i in range(10):\n",
    "    array = np.expand_dims(np.array(batch[i][\"image\"]), axis=0)\n",
    "    label = np.array(batch[i][\"label\"])\n",
    "    # send request and fetch prediction\n",
    "    payload  = {\"array\": array.tolist()}\n",
    "    response = requests.post(deployment.url, json=payload)\n",
    "    result = response.json()[0]\n",
    "    idx, cls = my_utils.to_prediction_cls(result)\n",
    "    matched = idx == label\n",
    "    my_utils.img_show(batch[i][\"image\"])\n",
    "    print(f\"prediction: {idx}; class: {cls}; matched: {matched}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown our server\n",
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this end-to-end ML application, you walked through a step-by-step guide on how to use Ray AIR library and its various components at each stage of an ML workflow and its respective workload: from data preprocessing and ingestion, training and tuning, to batch inference, and finaly deploying it for online inference. For this image classificaiton use case, we use PyTorch and its relevant AIR integrations such\n",
    "as TorchTrainer, TorchCheckpoint, TorchPredictor, etc. AIR supports other common frameworks such as TensorFlow, XGBoost, Scikit-learn, HugginFace etc.\n",
    "\n",
    "While we used fixed set of hyperparameters, try training and tuning with some different parameters in the exercise below.\n",
    "\n",
    "**Note**: On single machine, an M1 Mac laptop with 10 cores, this took over 15+ hours to train. On an Anyscale Ray cluster with 4 nodes and 32 GPUs, it was matter of minutes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    " * Try scaling your Trainer and Tunner\n",
    "  * increase the `num_workers` to 3 or 4\n",
    "  * extend the tuning space \n",
    "    * `epochs=[75, 100, 125]`\n",
    "    * `batch_size=[64, 128]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework \n",
    " * Read the resources below\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "Here some extra reading and resources for you:\n",
    " * Watch the Ray Summit 2022 on [Introduction to Ray AIR](https://www.anyscale.com/ray-summit-2022/agenda/sessions/226)\n",
    " * Ray AIR [documentation](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n",
    " * Understand its [Components and APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html)\n",
    " * Ray AIR [user guides](https://docs.ray.io/en/latest/ray-air/user-guides.html) and [examples](https://docs.ray.io/en/latest/ray-air/examples/index.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2706dab362c94031065b1e6b4ba35ae246b1d7f08f9a1709d5e0cd568b23fcbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
